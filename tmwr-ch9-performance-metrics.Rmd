
---
title: "Tidy Modeling with R: Chapter 9 (Model Performance)"
output:
  html_document: default
  pdf_document: default
---

```{r performance-setup, include = FALSE}
library(tidymodels)
library(tidyverse)
tidymodels_prefer()
```

This notebook works through the contents of chapter 9 of the book TMwR on model performance.

The data used is the Ames house sale price dataset, which is available from the `tidymodels` package.
```{r data}
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)
```

This chapter will demonstrate the `yardstick` package, a core tidymodels packages with the focus of measuring model performance.

# Regression Metrics

Two common metrics for regression models are the root mean squared error (RMSE) and the coefficient of determination (a.k.a. $R^2$). The former measures accuracy while the latter measures correlation. 

We use the linear regression workflow developed in chapter 8:
```{r ames_wf}
ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)
```

Let's see how well the model predicts the sales price of the houses in the test dataset. We start by making predictions using test set data.

```{r}
ames_test_results <- predict(lm_fit, new_data=ames_test %>% select(-Sale_Price))
ames_test_results
```

Let's add the actual sales prices to the results frame.

```{r}
ames_test_results <- bind_cols(ames_test_results, ames_test %>% select(Sale_Price))
ames_test_results
```

Then plot the actual versus predicted sale prices.

```{r}
ggplot(ames_test_results, aes(x = Sale_Price, y = .pred)) + 
  # Create a diagonal line:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  labs(y = "Predicted Sale Price (log10)", x = "Sale Price (log10)") +
  # Scale and size the x- and y-axis uniformly:
  coord_obs_pred()
```

We can measure model effectiveness with the root mean squared error (RMSE). The smaller the RMSE, the more accurate the model's predictions are.
```{r}
rmse(ames_test_results, truth = Sale_Price, estimate = .pred)
```

We can compute multiple metrics with the `metric_set()` function. Let's compute RMSE, R^2, and the mean absolute error (MAE).
```{r}
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_results, truth = Sale_Price, estimate = .pred)
```

# Binary Classification Metrics

Let's use the binary classification dataset from the `modeldata` package. The second and third columns are predicted class probabilities for the test set while predicted are predictions of which class the datum belongs to.

```{r}
data(two_class_example)
tibble(two_class_example)
```

The confusion matrix shows the number of true positives and true negatives on the diagonal and the number of false positives and false negatives off diagonal. 
```{r}
conf_mat(two_class_example, truth = truth, estimate = predicted)
```

Individual metrics can be computed with functions matching their names, which take the data frame containing the true and predicted values as an argument, along with the name of the columns where the actual and predicted values are stored.
```{r}
accuracy(two_class_example, truth, predicted)
```

Multiple metrics can be computed by constructing a `metric_set` as with regression metrics.
```{r}
classification_metrics <- metric_set(accuracy, precision, recall, f_meas)
classification_metrics(two_class_example, truth=truth, estimate=predicted)
```

For binary classification data sets, `yardstick` functions have a standard argument called `event_level` to distinguish positive and negative levels. If this argument is not provided, `yardstick` defaults to treating the first level of the outcome factor as the positive event of interest. Note the differences in precision, recall, and F-measure when we choose the second level (Class2) as the positive outcome.

```{r}
classification_metrics(two_class_example, truth=truth, estimate=predicted, event_level="second")
```

The receiver operating characteristic (ROC) curve computes the sensitivity and specificity over a continuum of different event thresholds. 

```{r}
two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve
```

We can plot the ROC curve with `autoplot()`.

```{r}
autoplot(two_class_curve)
```

Yardstick supports other curve types, like the precision-recall (PR) curve.

```{r}
pr_curve <- pr_curve(two_class_example, truth, Class1)
pr_curve
```

We can plot the PR curve with `autoplot()`.

```{r}
autoplot(pr_curve)
```

# Multiclass Classification Metrics

We will use the HPC dataset for multiclass classification.
```{r}
data(hpc_cv)
tibble(hpc_cv)
```

Performance metric functions for multiclass classification have are the same as those for binary classification, where that makes sense.
```{r}
accuracy(hpc_cv, obs, pred) 
```

Some metrics like `sensitivity` measure results based on the positive outcome. To apply such metrics to multiclass data, we need to choose a way to extend the concept of a positive class to such data. There are three options:

  - Macro-averaging computes a set of one-versus-all metrics using the standard two-class statistics. These are averaged.
  - Macro-weighted averaging does the same but the average is weighted by the number of samples in each class.
  - Micro-averaging computes the contribution for each class, aggregates them, then computes a single metric from the aggregates.

```{r}
sensitivity(hpc_cv, obs, pred, estimator="macro")
```

```{r}
sensitivity(hpc_cv, obs, pred, estimator="macro_weighted")
```

```{r}
sensitivity(hpc_cv, obs, pred, estimator="micro")
```

There are multiclass analogs of probability-based metrics like ROC AUC too. All of the class probability columns must be given to the function:

```{r}
roc_auc(hpc_cv, obs, VF, F, M, L)
```

We can also visualize the ROC curve using a 1-vs-all approach, where the 1 represents the positive class. In this example, we plot the ROC curve four times, one for each of the possible classes. Since the `hpc` dataset contains folds for cross-validation, we group by `Resample` to show the ROC curve for each fold.

```{r}
hpc_cv %>% 
  group_by(Resample) %>% 
  roc_curve(obs, VF, F, M, L) %>% 
  autoplot()
```

# References

  1. Max Kuhn and Julia Silge, _Tidy Modeling with R_, chapter 9. https://www.tmwr.org/performance.html (2022)
